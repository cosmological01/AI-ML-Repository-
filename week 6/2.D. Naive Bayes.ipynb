{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier ‚Äì Detailed Theory \n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Naive Bayes?\n",
    "\n",
    "Naive Bayes is a **probabilistic supervised learning algorithm** used for **classification**.  \n",
    "It is based on **Bayes‚Äô Theorem** and makes a simplifying assumption that the **features are conditionally independent given the class**.\n",
    "\n",
    "Despite this strong assumption, Naive Bayes performs very well in many real-world tasks, especially:\n",
    "- Text classification\n",
    "- Spam detection\n",
    "- Document categorization\n",
    "- Medical diagnosis (baseline models)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bayes‚Äô Theorem\n",
    "\n",
    "Bayes‚Äô theorem describes how to update the probability of a hypothesis when new evidence is observed.\n",
    "\n",
    "$$\n",
    "P(y \\mid X) = \\frac{P(X \\mid y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(y \\mid X)$ ‚Üí **Posterior probability** (probability of class given data)\n",
    "- $P(X \\mid y)$ ‚Üí **Likelihood**\n",
    "- $P(y)$ ‚Üí **Prior probability of the class**\n",
    "- $P(X)$ ‚Üí **Evidence** (constant for all classes)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Classification Using Bayes‚Äô Theorem\n",
    "\n",
    "In classification, we compute the posterior probability for **each class** and choose the class with the highest probability.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y P(X \\mid y) \\cdot P(y)\n",
    "$$\n",
    "\n",
    "Since $P(X)$ is the same for all classes, it can be ignored during comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. The ‚ÄúNaive‚Äù Independence Assumption\n",
    "\n",
    "Naive Bayes assumes that **features are conditionally independent given the class**.\n",
    "\n",
    "$$\n",
    "P(X \\mid y) = P(x_1, x_2, ..., x_n \\mid y)\n",
    "$$\n",
    "\n",
    "Naive assumption:\n",
    "\n",
    "$$\n",
    "P(X \\mid y) = \\prod_{i=1}^{n} P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "This simplifies computation greatly and makes the algorithm fast and scalable.\n",
    "\n",
    "> Even when this assumption is not perfectly true, Naive Bayes often works surprisingly well.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes is used when features are **continuous** (real-valued), such as:\n",
    "- Height\n",
    "- Weight\n",
    "- Sepal length (Iris dataset)\n",
    "\n",
    "It assumes each feature follows a **normal (Gaussian) distribution** within each class.\n",
    "\n",
    "### Gaussian Probability Density Function\n",
    "\n",
    "$$\n",
    "P(x \\mid y) =\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma_y^2}}\n",
    "\\exp\\left(\n",
    "-\\frac{(x - \\mu_y)^2}{2\\sigma_y^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_y$ ‚Üí Mean of feature for class $y$\n",
    "- $\\sigma_y^2$ ‚Üí Variance of feature for class $y$\n",
    "\n",
    "Each feature has its **own mean and variance per class**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Training Phase (What the Model Learns)\n",
    "\n",
    "For each class $y$, the model computes:\n",
    "\n",
    "1. **Prior probability**\n",
    "$$\n",
    "P(y) = \\frac{\\text{Number of samples in class } y}{\\text{Total samples}}\n",
    "$$\n",
    "\n",
    "2. **Mean of each feature**\n",
    "$$\n",
    "\\mu_{y,i} = \\text{mean of feature } i \\text{ for class } y\n",
    "$$\n",
    "\n",
    "3. **Variance of each feature**\n",
    "$$\n",
    "\\sigma_{y,i}^2 = \\text{variance of feature } i \\text{ for class } y\n",
    "$$\n",
    "\n",
    "No iterative optimization is needed. Training is very fast.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Prediction Phase\n",
    "\n",
    "For a new sample $X = (x_1, x_2, ..., x_n)$:\n",
    "\n",
    "1. Compute likelihood for each feature using Gaussian PDF\n",
    "2. Multiply all feature likelihoods\n",
    "3. Multiply by class prior\n",
    "4. Choose the class with maximum posterior probability\n",
    "\n",
    "$$\n",
    "P(y \\mid X) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Log Probabilities (Numerical Stability)\n",
    "\n",
    "Multiplying many small probabilities can cause numerical underflow.\n",
    "\n",
    "To avoid this, we use logarithms:\n",
    "\n",
    "$$\n",
    "\\log P(y \\mid X) = \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "Log does not change the class ordering and is numerically stable.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Predicted Probabilities\n",
    "\n",
    "Naive Bayes can output **class probabilities**, not just class labels.\n",
    "\n",
    "$$\n",
    "P(y = k \\mid X)\n",
    "$$\n",
    "\n",
    "These probabilities:\n",
    "- Provide confidence of predictions\n",
    "- Are useful in decision-making systems\n",
    "- Help understand model behavior\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Variants of Naive Bayes\n",
    "\n",
    "### 1. Gaussian Naive Bayes\n",
    "- Continuous features\n",
    "- Assumes normal distribution\n",
    "- Used in Iris, medical data\n",
    "\n",
    "### 2. Multinomial Naive Bayes\n",
    "- Discrete counts\n",
    "- Word frequency in text\n",
    "- Used in NLP and spam detection\n",
    "\n",
    "### 3. Bernoulli Naive Bayes\n",
    "- Binary features (0/1)\n",
    "- Presence or absence of a word\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Advantages of Naive Bayes\n",
    "\n",
    "- Very fast training and prediction\n",
    "- Works well with small datasets\n",
    "- Handles high-dimensional data\n",
    "- Performs well in text classification\n",
    "- Simple and interpretable\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Limitations of Naive Bayes\n",
    "\n",
    "- Strong independence assumption\n",
    "- Cannot model feature interactions\n",
    "- Poor probability calibration\n",
    "- Gaussian assumption may not fit real data perfectly\n",
    "\n",
    "---\n",
    "\n",
    "## 13. When to Use Naive Bayes\n",
    "\n",
    "Use Naive Bayes when:\n",
    "- You need a fast baseline model\n",
    "- Dataset is small or high-dimensional\n",
    "- Features are approximately independent\n",
    "- Text classification tasks\n",
    "\n",
    "Avoid when:\n",
    "- Features are highly correlated\n",
    "- Complex decision boundaries are required\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Summary\n",
    "\n",
    "- Naive Bayes is a probabilistic classifier\n",
    "- Based on Bayes‚Äô theorem\n",
    "- Assumes conditional independence\n",
    "- Gaussian NB handles continuous features\n",
    "- Simple, fast, and surprisingly effective\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Next Learning Steps\n",
    "\n",
    "- Implement Multinomial Naive Bayes for text\n",
    "- Compare Naive Bayes vs Logistic Regression\n",
    "- Study probability calibration\n",
    "- Apply Naive Bayes on real NLP datasets\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X,y = iris.data, iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      " P(setosa) = 0.000000\n",
      " P(versicolor) = 0.995636\n",
      " P(virginica) = 0.004364\n",
      "Predict class: versicolor\n",
      "\n",
      "Sample 2\n",
      " P(setosa) = 1.000000\n",
      " P(versicolor) = 0.000000\n",
      " P(virginica) = 0.000000\n",
      "Predict class: setosa\n",
      "\n",
      "Sample 3\n",
      " P(setosa) = 0.000000\n",
      " P(versicolor) = 0.000000\n",
      " P(virginica) = 1.000000\n",
      "Predict class: virginica\n",
      "\n",
      "Sample 4\n",
      " P(setosa) = 0.000000\n",
      " P(versicolor) = 0.977593\n",
      " P(virginica) = 0.022407\n",
      "Predict class: versicolor\n",
      "\n",
      "Sample 5\n",
      " P(setosa) = 0.000000\n",
      " P(versicolor) = 0.870021\n",
      " P(virginica) = 0.129979\n",
      "Predict class: versicolor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def fit(self,X,y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean    = {}\n",
    "        self.var     = {}\n",
    "        self.prior   = {}\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            self.mean[cls] = X_c.mean(axis=0)\n",
    "            self.var[cls]  = X_c.var(axis=0)\n",
    "            self.prior[cls]= X_c.shape[0]/X.shape[0]\n",
    "            \n",
    "    def gaussian_pdf(self,x,mean,var):\n",
    "        eps = 1e-6\n",
    "        coeff = 1/np.sqrt(2*np.pi*var + eps)\n",
    "        exponent = np.exp(-((x-mean)**2)/(2*var+eps))\n",
    "        return coeff*exponent\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        probabilities = []\n",
    "        \n",
    "        for x in X:\n",
    "            class_probs = []\n",
    "            for cls in self.classes:\n",
    "                prior = np.log(self.prior[cls])\n",
    "                likelihood = np.sum(\n",
    "                    np.log(self.gaussian_pdf(x,self.mean[cls],self.var[cls]))\n",
    "                )\n",
    "                posterior = prior + likelihood\n",
    "                class_probs.append(posterior)\n",
    "                \n",
    "            # Convert log-probabilities to probabilities\n",
    "            class_probs = np.exp(class_probs)\n",
    "            class_probs = class_probs/np.sum(class_probs)\n",
    "            probabilities.append(class_probs)\n",
    "        return np.array(probabilities)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs,axis=1)\n",
    "    \n",
    "# Train the Model\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train,y_train)\n",
    "# Make Predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "# Observe Predicted Probabilities \n",
    "probs = gnb.predict_proba(X_test)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}\")\n",
    "    for cls, p in zip(class_names,probs[i]):\n",
    "        print(f\" P({cls}) = {p:4f}\")\n",
    "    print(\"Predict class:\",class_names[y_pred[i]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 1. Multinomial Naive Bayes (Text / Count Data)\n",
    "When to use\n",
    "- Word counts\n",
    "- Document-term matrices\n",
    "- Bag-of-Words / TF (not TF-IDF ideally)\n",
    "\n",
    "### Theory \n",
    "$$P(y|x) ‚àù P(y) ‚àè_{i} P(x_{i}|y)$$\n",
    "\n",
    "For multinomial NB:\n",
    "$$P(x_{i}|y) = \\frac{count(x_{i},y)}{\\sum count(y) + \\alpha n}$$\n",
    "(Laplace smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self,alpha=1.0):\n",
    "        self.alpha = alpha # Lapace smoothing \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.feature_log_prob = {}\n",
    "        self.class_log_prior = {}\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            class_count = X_c.shape[0]\n",
    "            \n",
    "            # Prior\n",
    "            self.class_log_prior[cls] = np.log(class_count/X.shape[0])\n",
    "            \n",
    "            # Feature probabilities with Laplace smoothing\n",
    "            feature_count = X_c.sum(axis=0) + self.alpha\n",
    "            total_count = feature_count.sum()\n",
    "            \n",
    "            self.feature_log_prob[cls] = np.log(feature/total_count)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        predictions = []\n",
    "        \n",
    "        for x in X:\n",
    "            class_scores = []\n",
    "            for cls in self.classes:\n",
    "                score = (\n",
    "                    self.class_log_prior[cls] \n",
    "                    + np.sum(x*self.feature_log_prob[cls])\n",
    "                )\n",
    "                class_scores.append(score)\n",
    "                \n",
    "            predictions.append(self.classes[np.argmax(class_scores)])\n",
    "            \n",
    "        return np.array(predictions)\n",
    "    \n",
    "X = np.array([\n",
    "    [3,0,1],\n",
    "    [2,0,0],\n",
    "    [1,3,0],\n",
    "    [0,2,1]\n",
    "])\n",
    "\n",
    "y = np.array([0,0,1,1])\n",
    "mnb = MultinomialNaiveBayes(alpha=1.0)\n",
    "\n",
    "print(mnb.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Laplace smoothing \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.feature_log_prob = {}\n",
    "        self.class_log_prior = {}\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            \n",
    "            # 1. Calculate Class Prior: P(c)\n",
    "            # Log(count of samples in class / total samples)\n",
    "            self.class_log_prior[cls] = np.log(X_c.shape[0] / n_samples)\n",
    "            \n",
    "            # 2. Calculate Feature Likelihoods with Laplace Smoothing: P(w|c)\n",
    "            # Numerator: Count of each feature in class + alpha\n",
    "            feature_counts = X_c.sum(axis=0) + self.alpha\n",
    "            # Denominator: Total count of all features in class + (alpha * n_features)\n",
    "            total_count = feature_counts.sum()\n",
    "            \n",
    "            # Store log probabilities to prevent underflow\n",
    "            self.feature_log_prob[cls] = np.log(feature_counts / total_count)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # We calculate: log(P(c)) + sum(feature_counts * log(P(w|c)))\n",
    "        class_scores = []\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            # Vectorized calculation using matrix multiplication (@)\n",
    "            # This calculates the score for all rows in X simultaneously\n",
    "            prior = self.class_log_prior[cls]\n",
    "            likelihood = X @ self.feature_log_prob[cls]\n",
    "            class_scores.append(prior + likelihood)\n",
    "        \n",
    "        # Convert list to array (shape: n_classes, n_samples)\n",
    "        # argmax along axis 0 gives the index of the highest score per sample\n",
    "        class_scores = np.array(class_scores)\n",
    "        return self.classes[np.argmax(class_scores, axis=0)]\n",
    "\n",
    "# --- Testing the implementation ---\n",
    "X = np.array([\n",
    "    [3, 0, 1], # Sample 0\n",
    "    [2, 0, 0], # Sample 1\n",
    "    [1, 3, 0], # Sample 2\n",
    "    [0, 2, 1]  # Sample 3\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "mnb = MultinomialNaiveBayes(alpha=1.0)\n",
    "mnb.fit(X, y)\n",
    "\n",
    "predictions = mnb.predict(X)\n",
    "print(f\"Predictions: {predictions}\")\n",
    "# Expected Output: [0 0 1 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Bernoulli Naive Bayes (From Scratch)\n",
    "Used for:\n",
    "\n",
    "Binary features\n",
    "\n",
    "Spam detection, keyword presence\n",
    "\n",
    "Likelihood:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) =\n",
    "\\begin{cases}\n",
    "p_{iy}, & \\text{if } x_i = 1 \\\\\n",
    "1 - p_{iy}, & \\text{if } x_i = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### üîπ Bernoulli NB Code (From Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.feature_prob = {}\n",
    "        self.class_log_prior = {}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            self.class_log_prior[cls] = np.log(X_c.shape[0] / X.shape[0])\n",
    "\n",
    "            # Probability of feature being 1\n",
    "            feature_count = X_c.sum(axis=0)\n",
    "            self.feature_prob[cls] = (\n",
    "                feature_count + self.alpha\n",
    "            ) / (X_c.shape[0] + 2 * self.alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            class_scores = []\n",
    "            for cls in self.classes:\n",
    "                log_prob = self.class_log_prior[cls]\n",
    "                prob = self.feature_prob[cls]\n",
    "\n",
    "                log_prob += np.sum(\n",
    "                    x * np.log(prob) + (1 - x) * np.log(1 - prob)\n",
    "                )\n",
    "                class_scores.append(log_prob)\n",
    "\n",
    "            predictions.append(self.classes[np.argmax(class_scores)])\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    \n",
    "X = np.array([\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "bnb = BernoulliNaiveBayes(alpha=1.0)\n",
    "bnb.fit(X, y)\n",
    "\n",
    "print(bnb.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
