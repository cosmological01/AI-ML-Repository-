{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e609aa1",
   "metadata": {},
   "source": [
    "# Decision Trees â€“ Theoretical Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is a Decision Tree?\n",
    "\n",
    "A **Decision Tree** is a supervised machine learning algorithm used for both **classification** and **regression**.  \n",
    "It models decisions in the form of a **tree-like structure**, where:\n",
    "\n",
    "- **Root node** represents the entire dataset\n",
    "- **Internal nodes** represent feature-based decisions\n",
    "- **Branches** represent outcomes of those decisions\n",
    "- **Leaf nodes** represent final predictions (class labels or values)\n",
    "\n",
    "Decision trees work by **recursively splitting the data** into smaller and more homogeneous subsets.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Structure of a Decision Tree\n",
    "\n",
    "### 2.1 Nodes in a Tree\n",
    "\n",
    "- **Root Node**  \n",
    "  The topmost node where the first split happens.\n",
    "\n",
    "- **Decision / Internal Nodes**  \n",
    "  Nodes that split data based on a feature and threshold.\n",
    "\n",
    "- **Leaf Nodes**  \n",
    "  Terminal nodes that give the final output.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How Decision Trees Learn\n",
    "\n",
    "The learning process follows a **top-down greedy approach**:\n",
    "\n",
    "1. Choose the best feature to split the data.\n",
    "2. Split the dataset into subsets.\n",
    "3. Repeat recursively for each subset.\n",
    "4. Stop when:\n",
    "   - All samples belong to the same class, or\n",
    "   - Maximum depth is reached, or\n",
    "   - Minimum samples per node condition is met.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Splitting Criteria\n",
    "\n",
    "To decide the best split, decision trees use **impurity measures**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Gini Impurity\n",
    "\n",
    "### 5.1 Definition\n",
    "\n",
    "Gini impurity measures how often a randomly chosen element would be incorrectly classified.\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_i$ is the probability of class $i$\n",
    "- \\(C\\) is the number of classes\n",
    "\n",
    "### 5.2 Interpretation\n",
    "\n",
    "- Gini = 0 â†’ Pure node\n",
    "- Higher Gini â†’ More mixed classes\n",
    "\n",
    "### 5.3 Why Gini?\n",
    "\n",
    "- Faster computation\n",
    "- Commonly used in practice (e.g., CART algorithm)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Entropy and Information Gain\n",
    "\n",
    "### 6.1 Entropy\n",
    "\n",
    "Entropy measures the **amount of uncertainty** in the data.\n",
    "\n",
    "$$\n",
    "Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "- Entropy = 0 â†’ Pure node\n",
    "- Maximum when classes are equally distributed\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Information Gain\n",
    "\n",
    "Information Gain tells us how much uncertainty is reduced after a split.\n",
    "\n",
    "$$\n",
    "IG = Entropy(parent) - \\sum_{j} \\frac{n_j}{n} Entropy(child_{j})\n",
    "$$\n",
    "\n",
    "The feature with the **highest Information Gain** is chosen for splitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Comparison: Gini vs Entropy\n",
    "\n",
    "| Aspect | Gini | Entropy |\n",
    "|------|------|---------|\n",
    "| Basis | Probability of misclassification | Information theory |\n",
    "| Speed | Faster | Slower |\n",
    "| Split Quality | Similar | Slightly better sometimes |\n",
    "| Usage | Default in sklearn | Optional |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Decision Boundaries\n",
    "\n",
    "- Decision trees create **axis-aligned splits**\n",
    "- Resulting decision boundaries are **rectangular**\n",
    "- More depth â†’ more complex boundaries\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Overfitting in Decision Trees\n",
    "\n",
    "### Why Overfitting Happens\n",
    "\n",
    "- Trees can grow very deep\n",
    "- Can perfectly memorize training data\n",
    "- Sensitive to noise\n",
    "\n",
    "### Symptoms\n",
    "\n",
    "- Very high training accuracy\n",
    "- Lower test accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Controlling Overfitting\n",
    "\n",
    "### 10.1 Pre-Pruning (Early Stopping)\n",
    "\n",
    "- `max_depth`\n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `max_features`\n",
    "\n",
    "### 10.2 Post-Pruning\n",
    "\n",
    "- Remove branches after training\n",
    "- Cost-complexity pruning (advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Feature Importance in Decision Trees\n",
    "\n",
    "Decision trees naturally compute feature importance based on:\n",
    "\n",
    "- Reduction in impurity\n",
    "- Frequency of feature usage\n",
    "\n",
    "Higher importance â†’ feature contributes more to predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Advantages of Decision Trees\n",
    "\n",
    "- Easy to interpret and visualize\n",
    "- Handles non-linear relationships\n",
    "- No need for feature scaling\n",
    "- Works with numerical and categorical data\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Limitations of Decision Trees\n",
    "\n",
    "- Prone to overfitting\n",
    "- Unstable to small data changes\n",
    "- Axis-aligned splits only\n",
    "- Lower accuracy compared to ensembles\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Decision Trees vs Other Models\n",
    "\n",
    "| Model | Interpretability | Non-linearity | Scaling Needed |\n",
    "|-----|------------------|--------------|---------------|\n",
    "| Decision Tree | High | Yes | No |\n",
    "| Logistic Regression | Medium | No | Yes |\n",
    "| KNN | Low | Yes | Yes |\n",
    "| SVM | Low | Yes | Yes |\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Real-World Applications\n",
    "\n",
    "- Medical diagnosis\n",
    "- Credit approval\n",
    "- Customer churn prediction\n",
    "- Risk assessment\n",
    "- Rule-based decision systems\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Key Takeaways\n",
    "\n",
    "- Decision trees split data using impurity measures\n",
    "- Gini and Entropy are most common criteria\n",
    "- Trees can easily overfit if unrestricted\n",
    "- Limiting depth improves generalization\n",
    "- Visualization helps interpret decisions\n",
    "\n",
    "---\n",
    "\n",
    "## 17. What to Learn Next\n",
    "\n",
    "- Random Forests\n",
    "- Gradient Boosting Trees\n",
    "- XGBoost / LightGBM\n",
    "- Tree pruning techniques\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a34d1",
   "metadata": {},
   "source": [
    "### ðŸŒ³ Decision Tree Classifier (From Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a5a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch Tree Accuracy: 97.33%\n",
      "\n",
      "--- Tree Structure ---\n",
      "[petal length (cm) â‰¤ 1.90]\n",
      "  Leaf â†’ Class setosa\n",
      "  [petal width (cm) â‰¤ 1.70]\n",
      "    [petal length (cm) â‰¤ 4.90]\n",
      "      Leaf â†’ Class versicolor\n",
      "      Leaf â†’ Class virginica\n",
      "    [petal length (cm) â‰¤ 4.80]\n",
      "      Leaf â†’ Class virginica\n",
      "      Leaf â†’ Class virginica\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "class_names = data.target_names\n",
    "\n",
    "# 2. CORE MATHEMATICS: GINI IMPURITY\n",
    "def gini_impurity(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    return 1 - np.sum(probs**2)\n",
    "\n",
    "# 3. SPLIT LOGIC\n",
    "def split_dataset(X, y, feature, threshold):\n",
    "    left_mask = X[:, feature] <= threshold\n",
    "    right_mask = X[:, feature] > threshold\n",
    "    return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "def best_split(X, y):\n",
    "    best_gini = float(\"inf\")\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            X_left, y_left, X_right, y_right = split_dataset(X, y, feature, threshold)\n",
    "            \n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted Gini Impurity\n",
    "            w_left = len(y_left) / len(y)\n",
    "            w_right = len(y_right) / len(y)\n",
    "            current_gini = (w_left * gini_impurity(y_left)) + (w_right * gini_impurity(y_right))\n",
    "            \n",
    "            if current_gini < best_gini:\n",
    "                best_gini = current_gini\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                \n",
    "    # FIX: Return outside the loops so we check ALL features/thresholds\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "# 4. TREE ARCHITECTURE\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value # If value is not None, this is a Leaf Node\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3):\n",
    "    n_samples, n_features = X.shape\n",
    "    unique_classes = np.unique(y)\n",
    "    \n",
    "    # Stopping Criteria: Pure node, Max depth, or no samples\n",
    "    if len(unique_classes) == 1 or depth == max_depth or n_samples < 2:\n",
    "        return TreeNode(value=np.bincount(y).argmax())\n",
    "    \n",
    "    feature, threshold = best_split(X, y)\n",
    "    \n",
    "    # FIX: Correct syntax for None check\n",
    "    if feature is None:\n",
    "        return TreeNode(value=np.bincount(y).argmax())\n",
    "    \n",
    "    X_left, y_left, X_right, y_right = split_dataset(X, y, feature, threshold)\n",
    "    \n",
    "    # Recursive builds\n",
    "    left_child = build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "    right_child = build_tree(X_right, y_right, depth + 1, max_depth)\n",
    "    \n",
    "    return TreeNode(feature, threshold, left_child, right_child)\n",
    "\n",
    "# 5. PREDICTION\n",
    "def predict_sample(node, x):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    \n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return predict_sample(node.left, x)\n",
    "    return predict_sample(node.right, x)\n",
    "\n",
    "def predict(tree, X):\n",
    "    return np.array([predict_sample(tree, x) for x in X])\n",
    "\n",
    "# 6. RUN AND PRINT\n",
    "tree = build_tree(X, y, max_depth=3)\n",
    "preds = predict(tree, X)\n",
    "print(f\"Scratch Tree Accuracy: {np.mean(preds == y) * 100:.2f}%\")\n",
    "\n",
    "def print_tree(node, depth=0):\n",
    "    if node.value is not None:\n",
    "        print(\"  \" * depth + f\"Leaf â†’ Class {class_names[node.value]}\")\n",
    "        return\n",
    "\n",
    "    print(\"  \" * depth + f\"[{feature_names[node.feature]} â‰¤ {node.threshold:.2f}]\")\n",
    "    print_tree(node.left, depth + 1)\n",
    "    print_tree(node.right, depth + 1)\n",
    "\n",
    "print(\"\\n--- Tree Structure ---\")\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84b1dc",
   "metadata": {},
   "source": [
    "--------\n",
    "--------\n",
    "\n",
    "#### ðŸ“˜ How Decision Trees Work (Jupyter Markdown)\n",
    "1. Gini Impurity: Measuring \"Messiness\"\n",
    "\n",
    "The Decision Tree wants to split the data so that the resulting groups are as \"pure\" as possible. We use Gini Impurity to measure this.\n",
    "\n",
    "The Mathematical Formula:\n",
    "$$Gini = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "- If a node contains only one class, $Gini = 0$ (Perfectly pure).\n",
    "- If a node is a 50/50 mix of two classes, $Gini = 0.5$ (Maximum impurity).\n",
    "\n",
    "2. Recursive Splitting (Divide & Conquer)\n",
    "\n",
    "The build_tree function uses recursion. It behaves like a greedy algorithm:\n",
    "- At every node, it looks at every feature and every possible value to split on.\n",
    "- It chooses the split that reduces the total Gini impurity the most.\n",
    "- It repeats this process for the \"left\" and \"right\" subsets until it hits a Stopping Criterion.\n",
    "\n",
    "Common Stopping Criteria:\n",
    "\n",
    "- Max Depth: Prevents the tree from becoming too complex (overfitting).\n",
    "- Pure Node: All samples in the node belong to the same class.\n",
    "- Minimum Samples: Don't split if a node has too few data points.\n",
    "\n",
    "3. Decisions: Thresholds\n",
    "\n",
    "Unlike KNN (which uses distances) or Logistic Regression (which uses weights), a Decision Tree uses Thresholds. It creates axis-aligned boundaries in the feature space.\n",
    "\n",
    "- Example: If Petal Width <= 0.8, the Iris is almost certainly a Setosa.\n",
    "- The model doesn't care about the actual value of the Petal Width, only whether it is on the \"left\" or \"right\" of that specific number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbccaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f62f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d53ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d2a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5fe92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747dd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa680311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb9b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5ac52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72005c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813edc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f667bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99030f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a16b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051e1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750323c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd7de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
