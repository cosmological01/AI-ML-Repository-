{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. One function inside another (basic chain idea)\n",
    "Think of this as a two-step pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 11\n"
     ]
    }
   ],
   "source": [
    "#Step 1: inner function\n",
    "def g(x):\n",
    "    return x*3\n",
    "# Step 2: outer function\n",
    "def f(u):\n",
    "    return u+5\n",
    "x = 2\n",
    "u = g(x) # forward pass through g\n",
    "y = f(u) #forward pass through f\n",
    "\n",
    "print(\"Output:\",y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "* x affects u\n",
    "* u affects y\n",
    "* So x affects y through u\n",
    "\n",
    "### 2. “How much did x influence y?” (manual gradient flow)\n",
    "\n",
    "We track how sensitive each step is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y wrt x: 3\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "# forward \n",
    "u = x*3\n",
    "y = u + 5\n",
    "# backward (responsibility passing)\n",
    "dy_du = 1 # y changes 1-to-1 with u\n",
    "du_dx = 3 # u chnages 3-to-1 with x\n",
    "\n",
    "dy_dx = dy_du*du_dx\n",
    "\n",
    "print(\"Gradient of y wrt x:\",dy_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key idea\n",
    "* Each layer says: “If I change, how much do I affect the next thing?”\n",
    "* Multiply responsibilities as you go backward\n",
    "\n",
    "### 3. Tiny neural network (1 neuron, 2 layers)\n",
    "\n",
    "Now it starts to look like backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 9.0\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "x =1.5\n",
    "\n",
    "# parameters\n",
    "w1,w2 = 2.0, 3.0\n",
    "# forward pass\n",
    "h = x*w1 # layer 1\n",
    "y = h*w2 # layer 2\n",
    "print(\"Output:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass (intuition only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much x influenced y: 6.0\n"
     ]
    }
   ],
   "source": [
    "# final output sensitivity \n",
    "dy = 1\n",
    "# layer 2 responsibiility \n",
    "dy_dh = w2\n",
    "dh = dy*dy_dh\n",
    "\n",
    "# layer 1 responsibility \n",
    "dh_dx = w1\n",
    "dx =dh*dh_dx\n",
    "print(\"How much x influenced y:\",dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happened\n",
    "* Output says: “I depend on layer 2”\n",
    "* Layer 2 says: “I depend on layer 1”\n",
    "* Layer 1 says: “I depend on input”\n",
    "\n",
    "This is backpropagation.\n",
    "\n",
    "### 4. Add a non-linearity (real neural network behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 3.0\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "def relu(x):\n",
    "    return max(0,x)\n",
    "def relu_grad(x):\n",
    "    return 1 if x>0 else 0\n",
    "x = 2.0\n",
    "w = 1.5\n",
    "# forward\n",
    "z = x*w\n",
    "a = relu(z)\n",
    "\n",
    "print(\"Output:\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward (gradient flows only if active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt x: 1.5\n"
     ]
    }
   ],
   "source": [
    "da = 1 \n",
    "da_dz = relu_grad(z)\n",
    "dz_dx = w\n",
    "\n",
    "dx = da*da_dz*dz_dx\n",
    "print(\"Gradient wrt x:\",dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "* ReLU can block gradient flow\n",
    "* If a neuron is “off”, it passes no responsibility backward\n",
    "\n",
    "### 5. Multiple layers = chain rule in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 24.0\n"
     ]
    }
   ],
   "source": [
    "x = 1.0\n",
    "w1,w2,w3 = 2.0,3.0,4.0\n",
    "# forward\n",
    "a1 = x*w1\n",
    "a2 = a1*w2\n",
    "y = a2*w3\n",
    "\n",
    "print(\"Output:\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt x: 24.0\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "dy = 1\n",
    "dy_da2 =w3\n",
    "da2_da1 =w2\n",
    "da1_dx =w1\n",
    "\n",
    "dx = dy*dy_da2*da2_da1*da1_dx\n",
    "\n",
    "print(\"Gradient wrt x:\",dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mental model\n",
    "* Forward pass: values flow\n",
    "* Backward pass: blame flows\n",
    "* Each layer scales the blame\n",
    "\n",
    "### 6. Why this matters for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weight: 4.4\n"
     ]
    }
   ],
   "source": [
    "learning_rate =0.1\n",
    "w =2.0\n",
    "x =3.0\n",
    "# forward\n",
    "y = w*x\n",
    "loss = (y-10)**2\n",
    "# backward\n",
    "dloss_dy = 2*(y-10)\n",
    "dy_dw = x\n",
    "\n",
    "dw = dloss_dy * dy_dw\n",
    "w =w - learning_rate*dw\n",
    "\n",
    "print(\"Updated weight:\",w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "* Loss tells output it did something wrong\n",
    "* Output tells weight how responsible it was\n",
    "* Weight adjusts itself\n",
    "\n",
    "#### Practice next\n",
    "* Add one more layer and backprop manually\n",
    "* Replace ReLU with sigmoid and see gradient shrinking\n",
    "* Print gradients at each layer\n",
    "* Intentionally break gradient flow and observe learning stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
