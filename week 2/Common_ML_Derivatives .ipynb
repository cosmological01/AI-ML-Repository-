{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear function: $f(x)=Wx+b$\n",
    "This is the backbone of every neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 7.0\n",
      "Gradient wrt W: 2.0\n",
      "Gradient wrt x: 3.0\n",
      "Gradient wrt b: 1\n"
     ]
    }
   ],
   "source": [
    "x,W,b = 2.0,3.0,1.0\n",
    "# forward\n",
    "y = W*x +b\n",
    "print(\"Output\",y)\n",
    "\n",
    "# backward intuition\n",
    "# output changes directly with W,x, and b\n",
    "dy, dy_dw,dy_dx,dy_db = 1, x, W, 1\n",
    "print(\"Gradient wrt W:\",dy*dy_dw)\n",
    "print(\"Gradient wrt x:\",dy*dy_dx)\n",
    "print(\"Gradient wrt b:\",dy*dy_db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to notice\n",
    "* b always gets full responsibility\n",
    "* W is blamed proportional to input\n",
    "* This is why scaling inputs matters\n",
    "\n",
    "### 2. Sigmoid activation\n",
    "\n",
    "Sigmoid squashes values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid output: 0.7685247834990175\n",
      "Gradient wrt z: 0.17789444064680576\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    return 1/(1+math.exp(-z))\n",
    "z = 1.2\n",
    "# forward\n",
    "a = sigmoid(z)\n",
    "print(\"Sigmoid output:\",a)\n",
    "# backward intuition\n",
    "da_dz = a*(1-a)# sensitivity depends on output\n",
    "dz = 1\n",
    "print(\"Gradient wrt z:\",dz*da_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to notice\n",
    "* Near 0 or 1, gradient becomes very small\n",
    "* This explains vanishing gradients\n",
    "\n",
    "### 3. ReLU activation\n",
    "Simple, fast, and widely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU output: 0\n",
      "Gradient wrt z: 0\n"
     ]
    }
   ],
   "source": [
    "def relu(z):\n",
    "    return max(0,z)\n",
    "def relu_grad(z):\n",
    "    return 1 if z>0 else 0\n",
    "\n",
    "z = - 0.5\n",
    "# forward \n",
    "a = relu(z)\n",
    "print(\"ReLU output:\",a)\n",
    "# backward\n",
    "da = 1\n",
    "print(\"Gradient wrt z:\", da*relu_grad(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to notice\n",
    "* If neuron is off, it sends zero gradient\n",
    "* This can kill learning if many neurons stay inactive\n",
    "\n",
    "### 4. Softmax (intuition only)\n",
    "\n",
    "Softmax turns scores into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "scores = np.array([2.0,1.0,0.1])\n",
    "# forward\n",
    "exp_scores = np.exp(scores)\n",
    "probs = exp_scores/np.sum(exp_scores)\n",
    "\n",
    "print(\"Probabilities:\",probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient intuition\n",
    "* Increasing one score increases its probability\n",
    "* But decreases others\n",
    "* Outputs are coupled, unlike sigmoid or ReLU\n",
    "* Usually  softmax gradients  computed using Libraries.\n",
    "\n",
    "### 5. Mean Squared Error (MSE)\n",
    "\n",
    "Measures how wrong predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.0\n",
      "Gradient wrt prediction: -6.0\n"
     ]
    }
   ],
   "source": [
    "y_true = 10.0\n",
    "y_pred = 7.0\n",
    "# forward\n",
    "loss = (y_pred - y_true)**2\n",
    "print(\"Loss:\",loss)\n",
    "\n",
    "# backward \n",
    "dloss_dy = 2*(y_pred - y_true)\n",
    "\n",
    "print(\"Gradient wrt prediction:\",dloss_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to notice\n",
    "* Bigger error → bigger correction\n",
    "* Direction tells model whether to increase or decrease output\n",
    "\n",
    "### 6. Putting it together \n",
    "\n",
    "Linear + loss = learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update W: 5.2\n",
      "Update b: 1.4000000000000001\n"
     ]
    }
   ],
   "source": [
    "W, b, x,y_true, lr = 1.0,0.0, 3.0,10.0, 0.1\n",
    "# forward\n",
    "y_pred = W*x + b\n",
    "loss = (y_pred -y_true)**2\n",
    "# backward\n",
    "dloss_dy = 2*(y_pred-y_true)\n",
    "dy_dW = x\n",
    "dy_db = 1\n",
    "\n",
    "dW = dloss_dy*dy_dW\n",
    "db = dloss_dy*dy_db\n",
    "\n",
    "# update\n",
    "W -= lr*dW\n",
    "b -= lr*db\n",
    "\n",
    "print(\"Update W:\",W)\n",
    "print(\"Update b:\",b)\n",
    "# This is gradient descent in its simplest form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to practice effectively\n",
    "Try this:\n",
    "* Change inputs and see gradient signs flip\n",
    "* Stack Linear → ReLU → Linear → MSE\n",
    "* Print gradients at every step\n",
    "* Break ReLU (use negative input) and observe learning stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
