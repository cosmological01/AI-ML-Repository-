{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rank\n",
    "\n",
    "Rank = number of independent directions (information content)\n",
    "\n",
    "#### Basic intuition\n",
    "\n",
    "* Rank = how many features actually add new information\n",
    "\n",
    "* Low rank â†’ redundant or correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([\n",
    "    [1,2],\n",
    "    [2,4]\n",
    "])\n",
    "# Row 2 is a multiple of row 1 â†’ redundant.\n",
    "# Rank\n",
    "np.linalg.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML meaning\n",
    "#     Dataset has only 1 useful feature\n",
    "#     Linear regression matrix is singular\n",
    "#     PCA will collapse dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "* Compute rank of a random 3Ã—3 matrix\n",
    "* Remove a redundant column and compare ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Orthogonality \n",
    "* Vectors are orthogonal if dot product = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([1,0,0])\n",
    "v2 = np.array([0,1,0])\n",
    "v3 = np.array([0,0,1])\n",
    "\n",
    "np.dot(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML meaning\n",
    "* Orthogonal features â†’ no correlation\n",
    "* PCA creates orthogonal components\n",
    "* Helps avoid multicollinearity\n",
    "\n",
    "####  Practice\n",
    "* Check if [1, 2] and [2, -1] are orthogonal\n",
    "* Normalize orthogonal vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Span\n",
    "Span = all linear combinations of vectors\n",
    "\n",
    "v1,v2,v3 \n",
    "\n",
    "Their span = entire 3D space.\n",
    "\n",
    "### ML meaning\n",
    "* Features span the feature space\n",
    "* If span is limited â†’ model cannot learn patterns\n",
    "\n",
    "### Practice\n",
    "* Check if a vector lies in span of others\n",
    "* Use rank to verify span size\n",
    "\n",
    "### 4. Projection (core of linear regression)\n",
    "Intuition\n",
    "\n",
    "Projection = closest approximation of one vector onto another.\n",
    "\n",
    "Basic projection formula \n",
    "$$\\text{proj}_{v}(u) = \\frac{u.v}{v.v} v$$\n",
    "\n",
    "### Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "u = np.array([3,2,5])\n",
    "v = np.array([1,0,0])\n",
    "\n",
    "proj = (np.dot(u,v)/np.dot(v,v))*v\n",
    "print(proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML meaning\n",
    "* Predictions are projections onto feature space\n",
    "* Residuals are perpendicular to features\n",
    "\n",
    "#### 5. Projection onto multiple vectors(regression view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1,1],\n",
    "    [1,2],\n",
    "    [1,3]\n",
    "])\n",
    "\n",
    "y = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection matrix \n",
    "$$P=X(X^{T}X)^{-1}X^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P =  X@np.linalg.inv(X.T@X)@X.T\n",
    "y_hat = P@y\n",
    "y_hat\n",
    "# This is least squares regression geometrically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Orthogonality of residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0658141036401503e-14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual = y - y_hat\n",
    "np.dot(residual,X[:,1])\n",
    "# â‰ˆ 0 â†’ residuals orthogonal to feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ’¡ Advanced ML Interpretation Concepts\n",
    "\n",
    "This table outlines key linear algebra concepts and their practical meaning in machine learning, particularly in dimensionality reduction (like PCA) and linear models.\n",
    "\n",
    "| Concept | ML Interpretation / Meaning | Related to... |\n",
    "| :--- | :--- | :--- |\n",
    "| **Rank** | The **effective number of features** or dimensions required to describe the data without losing information. | Degrees of freedom, **Model Capacity** |\n",
    "| **Orthogonality** | Features (or components) are **independent** and contain unique information, meaning they lie at $90^\\circ$ angles in feature space. | **Feature Independence**, PCA |\n",
    "| **Span** | The **model capacity** or the space of all possible data points that can be generated by a linear combination of the feature vectors. | Basis vectors, Data Representation |\n",
    "| **Projection** | Transforming a data point onto a subspace (e.g., a Principal Component) to **predict** its value in that lower-dimensional space. | **Prediction**, Dimensionality Reduction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Concept|ML Meaning|\n",
    "|:---|:---|\n",
    "|Rank|Effective number of features|\n",
    "|Orthogonality|Feature independence|\n",
    "|Span|Model capacity|\n",
    "|Projection| Prediction|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. PCA connection\n",
    "* PCA finds orthogonal directions\n",
    "* Data projected onto lower-rank subspace\n",
    "* Rank reduces while preserving variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Practice exercises (do these)\n",
    "Basic\n",
    "* 1. Compute rank of a dataset matrix\n",
    "* 2. Check orthogonality of feature vectors\n",
    "* 3. Find span dimension using rank\n",
    "\n",
    "### Intermediate\n",
    "* 4. Project a point onto a line\n",
    "* 5. Visualize original vs projected vector\n",
    "* 6. Compute projection matrix\n",
    "\n",
    "### Advanced\n",
    "* 7. Show regression is projection\n",
    "* 8. Verify residuals are orthogonal\n",
    "* 9. Reduce rank using PCA\n",
    "* 10. Remove correlated features using rank analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
