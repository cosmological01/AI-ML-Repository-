{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dccbd23",
   "metadata": {},
   "source": [
    "# 1. Why partial derivatives matter in ML\n",
    "ML models have many parameters:\n",
    "* weights\n",
    "* biases\n",
    "* feature coefficients\n",
    "Partial derivatives answer:\n",
    "    “How does the loss change if I change one parameter, keeping others fixed?”\n",
    "\n",
    "That is the foundation of gradient descent.\n",
    "\n",
    "# 2. Core Idea (Must Understand)\n",
    "\n",
    "Given a function with multiple variables, the core concept of a partial derivative is to focus on one variable at a time.\n",
    "\n",
    "$$f(x, y)$$\n",
    "\n",
    "* $\\frac{\\partial f}{\\partial x} \\longrightarrow$ treat $y$ as **constant**\n",
    "* $\\frac{\\partial f}{\\partial y} \\longrightarrow$ treat $x$ as **constant**\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Basic Example (Given)\n",
    "\n",
    "Given the function:\n",
    "$$f(x, y) = x^2 + 3y$$\n",
    "\n",
    "### Manual Derivatives\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = 2x$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial y} = 3$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7ae58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n"
     ]
    }
   ],
   "source": [
    "def df_dx(x,y):\n",
    "    return 2*x\n",
    "def df_dy(x,y):\n",
    "    return 3\n",
    "print(df_dx(2,3),df_dy(2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e590bf",
   "metadata": {},
   "source": [
    "# 4. Gradient Vector (Very Important)\n",
    "\n",
    "The **gradient** collects all partial derivatives into a single vector. \n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eceeb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "def gradient(x,y):\n",
    "    return np.array([2*x,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778602b8",
   "metadata": {},
   "source": [
    "ML meaning:\n",
    "* gradient points in direction of steepest increase\n",
    "* gradient descent moves in opposite direction\n",
    "\n",
    "#### 5. $$f(x,y) = x^{2}+xy+y^{2}$$\n",
    "\n",
    "### Manual derivatives \n",
    "$$\\frac{\\partial f}{\\partial x} = 2x+y$$\n",
    "$$\\frac{\\partial f}{\\partial y} = x+2y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ad78af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_f(x,y):\n",
    "    df_dx = 2*x + y\n",
    "    df_dy = x + 2*y\n",
    "    return df_dx,df_dy\n",
    "grad_f(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d07ad",
   "metadata": {},
   "source": [
    "### 6. ML Loss Function example\n",
    "Mean Squeared Error(single data point):\n",
    "\n",
    "$$L(w,b) = (wx+b-y)^{2}$$\n",
    "\n",
    "Partial Derivatives\n",
    "$$\\frac{\\partial L}{\\partial w} = 2(wx+b−y)x$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = 2(wx+b−y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c746228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4, -2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_mse(w,b,x,y):\n",
    "    error = w*x+b-y\n",
    "    dL_dw = 2*error*x\n",
    "    dL_db = 2*error\n",
    "    return dL_dw, dL_db\n",
    "grad_mse(1,0,2,3)\n",
    "# This is exactly what linear regression uses internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc7838",
   "metadata": {},
   "source": [
    "### 7. Numerical partial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7657938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0000000000131024\n",
      "-4.000000000026205\n"
     ]
    }
   ],
   "source": [
    "def numerical_partial(f,x,y,var=\"x\",h=1e-5):\n",
    "    if var == 'x':\n",
    "        return (f(x+h,y)-f(x-h,y))/(2*h)\n",
    "    else:\n",
    "        return (f(x,y+h)-f(x,y-h))/(2*h)    \n",
    "    \n",
    "def f(x,y):\n",
    "    return x**2 - y**2\n",
    "\n",
    "print(numerical_partial(f,1,2,'x'))\n",
    "\n",
    "print(numerical_partial(f,1,2,'y'))\n",
    "\n",
    "# Used in:\n",
    "#     gradient checking\n",
    "#     debugging ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600d0c8",
   "metadata": {},
   "source": [
    "### 8. Gradient descent with partial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25359f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6, 0.8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = 0,0\n",
    "lr = 0.1\n",
    "x,y =  2,4\n",
    "\n",
    "for _ in range(10):\n",
    "    dw,db = grad_mse(w,b,x,y)\n",
    "    w -= lr*dw\n",
    "    b -= lr*db\n",
    "w,b\n",
    "# Shows learning via partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae221",
   "metadata": {},
   "source": [
    "# 9. Practice Exercises (Do These)\n",
    "\n",
    "### Basic (Multivariate)\n",
    "\n",
    "1.  **Compute partial derivatives of:**\n",
    "    $$f(x, y) = x^2 + y^2$$\n",
    "2.  **Evaluate gradient at $(1, 1)$** (using the partial derivatives from Q1).\n",
    "\n",
    "### Intermediate (Multivariate)\n",
    "\n",
    "3.  **Find partial derivatives of:**\n",
    "    $$f(x, y) = 3x^2y + 2y$$\n",
    "4.  **Verify using numerical approximation** (Compute the partial derivatives for $f(x, y) = 3x^2y + 2y$ using both the analytical method and a numerical method like the central difference formula, and compare the results).\n",
    "\n",
    "### Advanced \n",
    "* 5. Derive gradients of MSE for multiple data points\n",
    "* 6. Implement gradient descent manually\n",
    "* 7. Visualize loss surface and gradient direction\n",
    "* 8. Explain why gradients vanish at minima\n",
    "\n",
    "## Understand:\n",
    "* partial derivatives = parameter-wise sensitivity\n",
    "* gradient = learning signal\n",
    "* holding others constant = isolate effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8843dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e20d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae22e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d6f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79441248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714451dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
