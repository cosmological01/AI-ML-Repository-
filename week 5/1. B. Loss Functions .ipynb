{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1061ae1f",
   "metadata": {},
   "source": [
    "# üìâ Loss Functions: A Theoretical Guide\n",
    "\n",
    "Loss functions are at the heart of machine learning. They tell us **how wrong a model‚Äôs predictions are** and provide a direction for improving the model during training. This notebook explains the most commonly used **regression and classification loss functions** in a clear, conceptual way.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Is a Loss Function?\n",
    "\n",
    "A **loss function** maps a model‚Äôs prediction and the true target value to a **single numerical value** representing error.\n",
    "\n",
    "- **Lower loss** ‚Üí better predictions  \n",
    "- **Higher loss** ‚Üí worse predictions  \n",
    "\n",
    "During training, learning algorithms adjust model parameters to **minimize the loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Regression Loss Functions\n",
    "\n",
    "Regression problems deal with **continuous numerical outputs**, such as temperature, rainfall, or house prices.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Mean Squared Error (MSE)\n",
    "\n",
    "### Definition\n",
    "Mean Squared Error computes the **average of the squared differences** between predicted and actual values.\n",
    "\n",
    "### Mathematical Expression\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "- Squaring ensures all errors are positive.\n",
    "- Larger errors are **penalized heavily**.\n",
    "- Encourages predictions to be close to the true values.\n",
    "\n",
    "### Advantages\n",
    "- Smooth and differentiable.\n",
    "- Works well with gradient-based optimization.\n",
    "\n",
    "### Disadvantages\n",
    "- Highly sensitive to **outliers**.\n",
    "- A single large error can dominate the loss.\n",
    "\n",
    "### When to Use\n",
    "- When large errors must be strongly penalized.\n",
    "- Common in linear regression and neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Mean Absolute Error (MAE)\n",
    "\n",
    "### Definition\n",
    "Mean Absolute Error computes the **average absolute difference** between predictions and true values.\n",
    "\n",
    "### Mathematical Expression\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "- Measures error in the **same units** as the target variable.\n",
    "- Treats all errors equally.\n",
    "\n",
    "### Advantages\n",
    "- More robust to outliers than MSE.\n",
    "- Easy to interpret.\n",
    "\n",
    "### Disadvantages\n",
    "- Not differentiable at zero.\n",
    "- Slower convergence in gradient-based methods.\n",
    "\n",
    "### When to Use\n",
    "- When outliers are present.\n",
    "- When interpretability is important.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Huber Loss\n",
    "\n",
    "### Definition\n",
    "Huber Loss combines the strengths of **MSE and MAE**.\n",
    "\n",
    "- Behaves like **MSE** for small errors.\n",
    "- Behaves like **MAE** for large errors.\n",
    "\n",
    "### Mathematical Expression\n",
    "$$\n",
    "L_\\delta(a) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}a^2 & \\text{if } |a| \\le \\delta \\\\\n",
    "\\delta(|a| - \\frac{1}{2}\\delta) & \\text{if } |a| > \\delta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $ a = y - \\hat{y} $.\n",
    "\n",
    "### Advantages\n",
    "- Less sensitive to outliers than MSE.\n",
    "- Smooth and differentiable.\n",
    "\n",
    "### Disadvantages\n",
    "- Requires choosing the parameter $ \\delta $.\n",
    "\n",
    "### When to Use\n",
    "- When the data has some outliers but not extreme ones.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Classification Loss Functions\n",
    "\n",
    "Classification problems involve **discrete class labels**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 0/1 Loss\n",
    "\n",
    "### Definition\n",
    "0/1 Loss assigns:\n",
    "- **0** if prediction is correct\n",
    "- **1** if prediction is incorrect\n",
    "\n",
    "### Mathematical Expression\n",
    "$$\n",
    "L(y, \\hat{y}) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } y = \\hat{y} \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "- Measures **accuracy-based error**.\n",
    "- Ignores confidence of predictions.\n",
    "\n",
    "### Limitations\n",
    "- Not differentiable.\n",
    "- Cannot be used directly for training.\n",
    "\n",
    "### When to Use\n",
    "- For **evaluation**, not training.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Cross-Entropy (Log Loss)\n",
    "\n",
    "### Definition\n",
    "Cross-Entropy measures the difference between the **true probability distribution** and the **predicted probability distribution**.\n",
    "\n",
    "### Binary Classification Formula\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{n} \\sum \\left[y\\log(p) + (1-y)\\log(1-p)\\right]\n",
    "$$\n",
    "\n",
    "### Multiclass (Softmax) Version\n",
    "$$\n",
    "L = -\\sum_{k=1}^{K} y_k \\log(\\hat{y}_k)\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "- Strongly penalizes confident but wrong predictions.\n",
    "- Encourages probabilistic correctness.\n",
    "\n",
    "### Advantages\n",
    "- Differentiable and convex (for logistic regression).\n",
    "- Works well with probabilistic models.\n",
    "\n",
    "### Disadvantages\n",
    "- Sensitive to incorrect high-confidence predictions.\n",
    "\n",
    "### When to Use\n",
    "- Logistic regression\n",
    "- Neural networks\n",
    "- Softmax classifiers\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Different Loss Functions Matter\n",
    "\n",
    "| Problem Type | Preferred Loss | Reason |\n",
    "|--------------|---------------|--------|\n",
    "| Linear Regression | MSE | Penalizes large errors |\n",
    "| Robust Regression | MAE / Huber | Handles outliers |\n",
    "| Binary Classification | Log Loss | Probabilistic learning |\n",
    "| Multiclass Classification | Softmax Cross-Entropy | Probability distribution |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "- Loss functions define **what ‚Äúlearning‚Äù means**.\n",
    "- The choice of loss affects:\n",
    "  - Model behavior\n",
    "  - Sensitivity to outliers\n",
    "  - Training stability\n",
    "- There is **no universal best loss** ‚Äî it depends on the problem.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conceptual Link to Optimization\n",
    "\n",
    "Training algorithms (like Gradient Descent) do not directly maximize accuracy.  \n",
    "Instead, they **minimize a loss function**, and improved accuracy is a result of that minimization.\n",
    "\n",
    "---\n",
    "\n",
    "This theoretical foundation will help you understand:\n",
    "- Why certain models behave the way they do\n",
    "- How training objectives shape predictions\n",
    "- What loss function to choose for real-world problems\n",
    "\n",
    "-----------\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02fe234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.875\n",
      "MAE: 1.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Regression Loss Function (From Scratch) \n",
    "# Sample Dataset\n",
    "y_true = np.array([3,5,2.5,9])\n",
    "y_pred = np.array([2.5,6,4,7])\n",
    "\n",
    "# Mean Squared Error(MSE)\n",
    "def mse(y_true,y_pred):\n",
    "    return np.mean((y_true-y_pred)**2)\n",
    "print(\"MSE:\",mse(y_true,y_pred))\n",
    "\n",
    "# Mean Absolute Error(MAE)\n",
    "def mae(y_true,y_pred):\n",
    "    return np.mean(np.abs(y_true-y_pred))\n",
    "\n",
    "print(\"MAE:\",mae(y_true,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db29230",
   "metadata": {},
   "source": [
    "### Huber Loss \n",
    " Huber loss behaves like:\n",
    "- MSE for small errors\n",
    "- MAE for large errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f60616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hubber Loss: 0.78125\n"
     ]
    }
   ],
   "source": [
    "def huber_loss(y_true,y_pred,delta=1.0):\n",
    "    error = y_true-y_pred\n",
    "    is_small_error = np.abs(error) <=delta\n",
    "    \n",
    "    squared_loss = 0.5*error**2\n",
    "    linear_loss = delta*(np.abs(error)-0.5*delta)\n",
    "    \n",
    "    return np.mean(np.where(is_small_error,squared_loss,linear_loss))\n",
    "\n",
    "print(\"Hubber Loss:\", huber_loss(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839ad61",
   "metadata": {},
   "source": [
    "###  Classification Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc632ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Classification Data\n",
    "y_true_class = np.array([1, 0, 1, 1])\n",
    "y_pred_class = np.array([1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cfba448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1 Loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 0/1 Loss (Simple Classification Loss)\n",
    "def zero_one_loss(y_true,y_pred):\n",
    "    incorrect = y_true!=y_pred\n",
    "    return np.mean(incorrect)\n",
    "\n",
    "print(\"0/1 Loss:\",zero_one_loss(y_true_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6f41b",
   "metadata": {},
   "source": [
    "### Cross-Entropy/ Log Loss (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b81f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_prob = np.array([1,0,1,1])\n",
    "y_pred_prob = np.array([0.9,0.6,0.2,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e5b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.7135581778200728\n"
     ]
    }
   ],
   "source": [
    "def log_loss(y_true,y_pred):\n",
    "    epsilon = 1e-9  # to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    loss = -(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
    "    return np.mean(loss)\n",
    "print(\"Log Loss:\",log_loss(y_true_prob,y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48354907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770c495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403e485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df489f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212cb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67580aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
