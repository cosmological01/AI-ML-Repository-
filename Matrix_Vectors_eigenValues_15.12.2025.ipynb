{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7148d439",
   "metadata": {},
   "source": [
    "### 1. Matrix addition and subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f037ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C= [[2 4]\n",
      " [3 6]] \n",
      "\n",
      "D= [[0 0]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array(\n",
    "    [\n",
    "        [1,2],\n",
    "        [3,5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "B = np.array(\n",
    "    [\n",
    "        [1,2],\n",
    "        [0,1]\n",
    "    ]\n",
    ")\n",
    "print(\"C=\",A + B,\"\\n\")\n",
    "print(\"D=\",A - B)\n",
    "\n",
    "# Used in gradient updates\n",
    "# Error calculation\n",
    "# Weight adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ac7f0",
   "metadata": {},
   "source": [
    "### 3. Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e650a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4],\n",
       "       [ 3, 11]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = A@B\n",
    "C\n",
    "# ML relevance\n",
    "# Core of linear models\n",
    "# Neural network forward pass\n",
    "# Feature transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b47312",
   "metadata": {},
   "source": [
    "### 4. Identity Matrix \n",
    " Intuition\n",
    " Does nothing when multiplied\n",
    " Like number 1 for matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b79e41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 5.]]\n"
     ]
    }
   ],
   "source": [
    "I = np.eye(2)\n",
    "print(A@I)\n",
    "\n",
    "# ML relevance\n",
    "#     Regularization\n",
    "#     Matrix inversion\n",
    "#     Stability in optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae706d9",
   "metadata": {},
   "source": [
    "### 5. Transpose : Swap rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95530294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 5]] \n",
      "\n",
      "[[1 3]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "print(A,\"\\n\")\n",
    "print(A.T)\n",
    "# ML relevance\n",
    "#     Used in covariance matrices\n",
    "#     Dot products\n",
    "#     Backpropagation math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156af72d",
   "metadata": {},
   "source": [
    "### Determinant (scaling intitution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f55b5604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Intuition\n",
    "#     Measures how much a matrix scales space\n",
    "#     Zero determinant â†’ information lost\n",
    "\n",
    "print(np.linalg.det(A))\n",
    "\n",
    "# ML meaning\n",
    "#     det = 0 â†’ matrix not invertible\n",
    "#     Indicates collinearity\n",
    "#     Important in PCA and Gaussian models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf70054",
   "metadata": {},
   "source": [
    "### 7. Inverse (when it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f47f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.  2.]\n",
      " [ 3. -1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv)\n",
    "print(A@A_inv)\n",
    "\n",
    "# ML relevance\n",
    "#     Normal equation in linear regression\n",
    "#     Solving linear systems\n",
    "#     Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb879d",
   "metadata": {},
   "source": [
    "### Linear regression equation y = X w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2940345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [1.9],\n",
       "       [0.2]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [1,2],\n",
    "        [3,8],\n",
    "        [2,0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "w = np.array(\n",
    "    [\n",
    "        [0.1],\n",
    "        [0.2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = X@w\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4738775",
   "metadata": {},
   "source": [
    "### Normal Equation (closed-form regression) $$w = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e10b643e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1],\n",
       "       [0.2]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = (np.linalg.inv(X.T @ X)@X.T)@y\n",
    "w\n",
    "\n",
    "# Why this matters\n",
    "#     Shows why transpose, inverse, multiplication exist\n",
    "#     Explains why some matrices fail (singular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e049b",
   "metadata": {},
   "source": [
    "### Regularization idea (intuition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca699f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.92668024, -3.46232179],\n",
       "       [-3.46232179,  2.05702648]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_I = 0.1*np.eye(2)\n",
    "stable_inv = np.linalg.inv(A.T@A+lambda_I)\n",
    "\n",
    "stable_inv\n",
    "# This idea is used in Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e57eec",
   "metadata": {},
   "source": [
    "### Compute eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d159112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 5]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9a5493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16227766  6.16227766] \n",
      "\n",
      " [[-0.86460354 -0.36126098]\n",
      " [ 0.50245469 -0.93246475]]\n"
     ]
    }
   ],
   "source": [
    "values , vectors = np.linalg.eig(A)\n",
    "print(values,\"\\n\\n\", vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fec4a213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.14030584, -0.08153717]), array([ 0.14030584, -0.08153717]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ vectors[:,0] , values[0]*vectors[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dd1f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ML meaning of eigenvalues\n",
    "#     Large eigenvalue â†’ important direction\n",
    "#     Small eigenvalue â†’ less information\n",
    "#     Zero eigenvalue â†’ redundant feature\n",
    "# This is feature importance in linear algebra form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f8629",
   "metadata": {},
   "source": [
    "### 5. Covariance matrix (entry point to PCA)\n",
    "#### Sample dataset (2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "faf49d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.66666667, 1.66666667],\n",
       "       [1.66666667, 1.66666667]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2, 1],\n",
    "              [3, 2],\n",
    "              [4, 3],\n",
    "              [5, 4]])\n",
    "# Step 1: Center the data\n",
    "\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Always done before PCA\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "cov = np.cov(X_centered.T)\n",
    "cov\n",
    "\n",
    "# Covariance matrix shows:\n",
    "#     variance\n",
    "#     correlation between features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71185b09",
   "metadata": {},
   "source": [
    "### 6. Eigenvectors of covariance matrix (PCA core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65832d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.33333333 0.        ] \n",
      "\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "print(eigenvalues,\"\\n\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Interpretation\n",
    "#     Eigenvectors â†’ principal directions\n",
    "#     Eigenvalues â†’ variance captured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac423b",
   "metadata": {},
   "source": [
    "### 7. Choosing principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43495d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:,idx]\n",
    "\n",
    "# Largest eigenvalue â†’ first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f16f0",
   "metadata": {},
   "source": [
    "### 8. Project data onto principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "419c65ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.12132034, -0.70710678,  0.70710678,  2.12132034])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PC1 = eigenvectors[:,0]\n",
    "X_reduced = X_centered@PC1\n",
    "X_reduced\n",
    "# This is dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48408814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Visual intuition (important)\n",
    "# Before PCA:\n",
    "#     data spread in 2D\n",
    "# After PCA:\n",
    "#     data spread mostly along 1 direction\n",
    "# ML benefit:\n",
    "#     less noise\n",
    "#     faster models\n",
    "#     better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0e763",
   "metadata": {},
   "source": [
    "### 10. PCA using scikit-learn (real ML usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa2fb628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.12132034],\n",
       "       [ 0.70710678],\n",
       "       [-0.70710678],\n",
       "       [-2.12132034]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6bdbb293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f95cc9",
   "metadata": {},
   "source": [
    "# 12. Advanced ML insight (very important)\n",
    "\n",
    "## Covariance matrix: $\\Sigma = X^{T} X $\n",
    "\n",
    "## Eigenvectors of $\\Sigma$:\n",
    " *   define principal axes\n",
    " *   diagonalize the matrix\n",
    "\n",
    "## This connects PCA to:\n",
    " * SVD\n",
    " * linear regression\n",
    " * Gaussian distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955722de",
   "metadata": {},
   "source": [
    "# ðŸ’¡ Advanced ML Insight: Connecting PCA Concepts\n",
    "\n",
    "## The Covariance Matrix and Principal Axes\n",
    "\n",
    "The foundation of Principal Component Analysis (PCA) lies in the **Covariance Matrix**, $\\Sigma$.\n",
    "\n",
    "For a data matrix $X$ (where columns are features and rows are observations, assuming $X$ is centered), the covariance matrix is defined as:\n",
    "\n",
    "$$\\Sigma = X^{T} X$$\n",
    "\n",
    "### Eigen-Decomposition and Dimensionality Reduction\n",
    "\n",
    "The **eigenvectors** of the covariance matrix ($\\Sigma$) are crucial because they:\n",
    "* Define the **Principal Axes** (the directions of maximum variance in the data).\n",
    "* **Diagonalize the matrix**, transforming the data into a new coordinate system where the new features (principal components) are uncorrelated.\n",
    "\n",
    "---\n",
    "\n",
    "## PCA's Deep Connections\n",
    "\n",
    "This framework establishes deep mathematical connections between PCA and several fundamental machine learning and statistical concepts:\n",
    "\n",
    "* **Singular Value Decomposition (SVD):** The principal components of a data matrix $X$ are the **right singular vectors** of $X$ (or the eigenvectors of $X^TX$). The singular values are proportional to the square roots of the eigenvalues ($\\sigma_i = \\sqrt{\\lambda_i}$).\n",
    "* **Linear Regression:** PCA can be seen as finding the subspace that **best approximates** the data (minimizing the perpendicular distance to the subspace), which is closely related to the least squares minimization used in linear regression.\n",
    "* **Gaussian Distributions:** When data is assumed to follow a **multivariate Gaussian distribution**, the principal axes (eigenvectors) of the covariance matrix align with the axes of the Gaussian's elliptical contours, making PCA an optimal method for linear dimensionality reduction in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73cba289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvectors = directions of variance\n",
    "# eigenvalues = strength of variance\n",
    "# PCA = keep strongest directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043f160",
   "metadata": {},
   "source": [
    "# 13. Practice exercises (do these)\n",
    "\n",
    "Compute eigenvalues of:  [[3, 1],[1, 3]]\n",
    "\n",
    "\n",
    "* 1. Verify eigen equation A v = Î» v\n",
    "* 2. Create a dataset with 3 features and compute covariance\n",
    "* 3. Perform PCA manually using NumPy\n",
    "* 4. Reduce data from 3D â†’ 2D\n",
    "*  5.Compare with sklearn PCA output\n",
    "* 6. Plot original vs PCA-reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2d6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f0764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cf631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
