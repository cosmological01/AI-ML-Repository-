{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebb9b04",
   "metadata": {},
   "source": [
    "# Random Variables and Distributions (Core Concepts for Machine Learning)\n",
    "\n",
    "This notebook explains the foundational probability concepts used throughout machine learning, with clear intuition and ML-oriented interpretations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Random Variable\n",
    "\n",
    "A **random variable (RV)** is a numerical description of the outcome of a random process.\n",
    "\n",
    "In machine learning:\n",
    "- Data is assumed to be generated by an unknown random process.\n",
    "- Features and labels are modeled as random variables.\n",
    "\n",
    "**Examples**\n",
    "- Height of a person\n",
    "- Pixel intensity\n",
    "- Class label (spam / not spam)\n",
    "\n",
    "Random variables allow us to represent **uncertainty mathematically**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Discrete vs Continuous Random Variables\n",
    "\n",
    "### Discrete Random Variable\n",
    "\n",
    "A random variable is **discrete** if it takes **countable values**.\n",
    "\n",
    "**Examples**\n",
    "- Class labels: {0, 1}\n",
    "- Number of clicks\n",
    "- Word counts in NLP\n",
    "\n",
    "**Key property**\n",
    "- All possible values can be listed.\n",
    "\n",
    "**In ML**\n",
    "- Classification outputs\n",
    "- Topic IDs\n",
    "- Cluster labels\n",
    "\n",
    "---\n",
    "\n",
    "### Continuous Random Variable\n",
    "\n",
    "A random variable is **continuous** if it can take values from a continuous interval.\n",
    "\n",
    "**Examples**\n",
    "- Temperature\n",
    "- Rainfall amount\n",
    "- Height, weight, sensor readings\n",
    "\n",
    "**Key property**\n",
    "- Probability at a single point is zero.\n",
    "- Probabilities are defined over intervals.\n",
    "\n",
    "**In ML**\n",
    "- Input features\n",
    "- Regression targets\n",
    "- Latent variables\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Probability Mass Function (PMF)\n",
    "\n",
    "The **PMF** is used for **discrete random variables**.\n",
    "\n",
    "### Definition\n",
    "\\[\n",
    "P(X = x) = p(x)\n",
    "\\]\n",
    "\n",
    "### Properties\n",
    "- $0 \\le p(x) \\le 1$\n",
    "- $\\sum_x p(x) = 1$\n",
    "\n",
    "### Interpretation\n",
    "- Gives the probability of each discrete outcome.\n",
    "\n",
    "### ML Interpretation\n",
    "- Softmax outputs\n",
    "- Class prior probabilities\n",
    "- Label distributions\n",
    "\n",
    "**Example**\n",
    "\\[\n",
    "P(Y=0)=0.2, P(Y=1)=0.5, P(Y=2)=0.3\n",
    "\\]\n",
    "\n",
    "Loss functions like **cross-entropy** assume a PMF.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Probability Density Function (PDF)\n",
    "\n",
    "The **PDF** is used for **continuous random variables**.\n",
    "\n",
    "### Definition\n",
    "$$ f(x) \\ge 0,\\quad \\int_{-\\infty}^{\\infty} f(x)\\,dx = 1$$\n",
    "\n",
    "### Important Note\n",
    "- $f(x)$ is **not a probability**.\n",
    "- Probability is obtained by integration:\n",
    "\n",
    "$$ P(a \\le X \\le b) = \\int_a^b f(x)\\,dx $$\n",
    "\n",
    "### Why This Matters\n",
    "$$ P(X=x) = 0 $$\n",
    "for continuous variables.\n",
    "\n",
    "### ML Interpretation\n",
    "- Gaussian Naive Bayes\n",
    "- Gaussian Mixture Models\n",
    "- Density-based anomaly detection\n",
    "\n",
    "The PDF answers:\n",
    "> “How plausible is this value under the model?”\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Cumulative Distribution Function (CDF)\n",
    "\n",
    "The **CDF** applies to both discrete and continuous random variables.\n",
    "\n",
    "### Definition\n",
    "$$ F(x) = P(X \\le x)$$\n",
    "\n",
    "### Properties\n",
    "- Non-decreasing\n",
    "- $0 \\le F(x) \\le 1$\n",
    "- $F(-\\infty)=0,\\quad F(\\infty)=1$\n",
    "\n",
    "### Relation to PDF\n",
    "$$ F(x) = \\int_{-\\infty}^{x} f(t)\\,dt $$\n",
    "\n",
    "### Interpretation\n",
    "- Accumulated probability up to a value.\n",
    "\n",
    "### ML Interpretation\n",
    "- Threshold-based decisions\n",
    "- Percentiles and quantiles\n",
    "- Confidence estimation\n",
    "\n",
    "**Example**\n",
    "$$ F(1.0) = 0.84 $$\n",
    "\n",
    "Meaning: 84% of values lie below 1.0.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. How These Concepts Fit Together in ML\n",
    "\n",
    "### Classification\n",
    "- Labels → Discrete random variable\n",
    "- Model output → PMF\n",
    "- Loss → compares true label with predicted PMF\n",
    "\n",
    "### Regression / Feature Modeling\n",
    "- Features → Continuous random variables\n",
    "- Model assumption → PDF\n",
    "- Loss → negative log-likelihood\n",
    "\n",
    "### Decision Making\n",
    "- Scores → Random variables\n",
    "- Thresholds → CDF-based probabilities\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Random Variable**: models uncertainty\n",
    "- **Discrete RV + PMF**: probability of exact outcomes\n",
    "- **Continuous RV + PDF**: likelihood density\n",
    "- **CDF**: accumulated probability\n",
    "\n",
    "These concepts form the mathematical backbone of probabilistic machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e445cb",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "------\n",
    "\n",
    "### 1. Discrete random variable (PMF)\n",
    "\n",
    "Think of this as class probabilities in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5db81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample class labels: [1 2 1 1 2 2 2 2 2 1 1 1 1 1 0 0 1 1 1 1]\n",
      "PMF:\n",
      "P(Y=0)=0.2\n",
      "P(Y=1)=0.5\n",
      "P(Y=2)=0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Discrete random variable: class labels\n",
    "classes = np.array([0,1,2]) #e.g., three classes\n",
    "pmf = np.array([0.2,0.5,0.3]) # P(Y = class)\n",
    "\n",
    "# Smapling labels based on PMF\n",
    "samples  = np.random.choice(classes,size = 20, p =pmf)\n",
    "\n",
    "print(\"Sample class labels:\",samples)\n",
    "print(\"PMF:\")\n",
    "for c,p in zip(classes,pmf):\n",
    "    print(f\"P(Y={c})={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b9896",
   "metadata": {},
   "source": [
    "#### ML link:\n",
    "* This is exactly what a softmax output represents.\n",
    "* Cross-entropy loss assumes a categorical PMF.\n",
    "\n",
    "### 2. Continuous random variable (PDF)\n",
    "\n",
    "This mirrors feature distributions assumed in many models (Naive Bayes, GMMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f1eb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF at x=0: 0.3989422804014327\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Continuous random variable: a feature\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "\n",
    "x = np.linspace(-4,4,200)\n",
    "pdf = norm.pdf(x,mu,sigma)\n",
    "\n",
    "print(\"PDF at x=0:\",norm.pdf(0,mu,sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f86086",
   "metadata": {},
   "source": [
    "#### ML link:\n",
    "* Gaussian Naive Bayes models each feature using a PDF.\n",
    "* Density estimation is the foundation of anomaly detection.\n",
    "\n",
    "### 3. Cumulative Distribution Function (CDF)\n",
    "\n",
    "CDFs are used for probability thresholds, quantiles, and decision rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690ebaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X<= 1): 0.8413447460685429\n"
     ]
    }
   ],
   "source": [
    "cdf = norm.cdf(x,mu,sigma)\n",
    "print(\"P(X<= 1):\",norm.cdf(1,mu,sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a9607",
   "metadata": {},
   "source": [
    "#### ML link:\n",
    "* Used in probabilistic decision-making.\n",
    "* Helps convert raw scores into probabilities or confidence levels.\n",
    "\n",
    "### 4. Discrete vs Continuous together (real ML-style example)\n",
    "\n",
    "Binary classification with a continuous feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26624b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior scores:\n",
      "Class 0: 0.07771055739953504\n",
      "Class 1: 0.1408261307057198\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "# Binary class (discrete)\n",
    "y = np.array([0,1])\n",
    "# Feature distributions conditioned on class\n",
    "mu_0, sigma_0 = -1,1\n",
    "mu_1, sigma_1 = 1,1\n",
    "# New data point\n",
    "x_new = 0.5\n",
    "# Likelihoods (PDFs)\n",
    "p_x_given_0 = norm.pdf(x_new,mu_0,sigma_0)\n",
    "p_x_given_1 = norm.pdf(x_new,mu_1,sigma_1)\n",
    "\n",
    "# Class priors(PMF)\n",
    "p_0, p_1 = 0.6,0.4\n",
    "\n",
    "# Posterior (Bayes rule, unnormalized)\n",
    "posterior_0 = p_x_given_0 * p_0\n",
    "posterior_1 = p_x_given_1 * p_1\n",
    "\n",
    "print(\"Posterior scores:\")\n",
    "print(\"Class 0:\",posterior_0)\n",
    "print(\"Class 1:\",posterior_1)\n",
    "print(\"Predicted class:\",0 if posterior_0> posterior_1 else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee812d03",
   "metadata": {},
   "source": [
    "#### ML link:\n",
    "* This is Gaussian Naive Bayes in its simplest form.\n",
    "* Shows how PMF + PDF combine to make predictions.\n",
    "\n",
    "### Key intuition for ML\n",
    "* Discrete RV + PMF → labels, class probabilities, softmax\n",
    "* Continuous RV + PDF → features, likelihoods, density estimation\n",
    "* CDF → thresholds, confidence, decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a5209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f59fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb6d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b72ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37f112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17a284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210efe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52932ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35347119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15336d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0effd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
